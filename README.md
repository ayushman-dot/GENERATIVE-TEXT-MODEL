# GENERATIVE-TEXT-MODEL

*COMPANY*: CODETECH IT SOLUTIONS

*NAME*: AYUSHMAN JAISWAL

*INTERN ID*: CT04DG3348

*DOMAIN*: ARTIFICIAL INTELLIGENCE

*DURATION* : 4 WEEKS

*MENTOR*: NEELA SANTOSH


*DESCRIPTION*

This project demonstrates a simple but powerful text generation model using GPT-2, a state-of-the-art language model developed by OpenAI. The goal is to create a system that can take in a prompt from a user and generate a coherent, human-like paragraph in response. This technology can be applied in a wide range of real-world scenarios including content writing, chatbots, storytelling, educational tools, and more.

At the heart of this system is the Transformer architecture, which revolutionized natural language processing by enabling the model to understand the context of words in a sentence more effectively than previous models like RNNs or LSTMs. GPT-2 has been pre-trained on a large portion of the internet, learning syntax, grammar, facts, style, and reasoning patterns. This pre-training enables it to generate relevant and contextually appropriate text given a simple prompt.

In this notebook, we use the transformers library by Hugging Face, which makes loading and using models like GPT-2 extremely straightforward. The code first loads the tokenizer and model, and then defines a function generate_text() that takes a user prompt and produces a continuation. The generation process is based on sampling from a probability distribution, meaning each run may yield different but meaningful results.

*OUTPUT*

<img width="1781" height="99" alt="Image" src="https://github.com/user-attachments/assets/cbf79d32-8629-46db-8d2f-ebf2092a8c64" />

<img width="1770" height="73" alt="Image" src="https://github.com/user-attachments/assets/073fd260-f53a-4067-a4b5-d6b5ecbe1cba" />

<img width="1784" height="56" alt="Image" src="https://github.com/user-attachments/assets/6febe07f-64e8-40de-9592-e834030182f8" />
