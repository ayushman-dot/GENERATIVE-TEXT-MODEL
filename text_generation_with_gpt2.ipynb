{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0570dea7",
   "metadata": {},
   "source": [
    "# Text Generation with GPT-2\n",
    "This notebook demonstrates how to use a pre-trained GPT-2 model to generate text based on custom prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f51a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598774b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb68eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(prompt, max_length=200):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=max_length, do_sample=True, top_k=50, top_p=0.95, temperature=0.8)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4862625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try it out!\n",
    "prompt = \"The future of artificial intelligence in education is\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f000f",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“„ Description (Humanized)\n",
    "\n",
    "This project showcases a text generation system using a pre-trained GPT-2 language model. Language models like GPT-2 are designed to generate human-like text by predicting the next word based on the previous ones. In this notebook, we use the Transformers library developed by Hugging Face to load and run GPT-2 easily.\n",
    "\n",
    "When a user enters a text prompt like *\"The future of artificial intelligence in education is...\"*, the model continues the sentence by generating coherent and relevant paragraphs. It does this using deep learning techniques, having been trained on a large portion of the internet to learn grammar, context, facts, and even tone.\n",
    "\n",
    "This tool can be used for creative writing, automatic article generation, conversation simulation, and much more. You can change the prompt and run the generation multiple times to get different responses each time.\n",
    "\n",
    "Behind the scenes, the model uses attention mechanisms and a transformer architecture to weigh the importance of each word in a sequence, making the output highly context-aware. Though GPT-2 was trained up to 2019 data, it still captures a great deal of general knowledge.\n",
    "\n",
    "You can experiment with parameters like `max_length`, `temperature`, and `top_k` to adjust how creative, focused, or random the responses are.\n",
    "\n",
    "This notebook provides a powerful, flexible, and simple way to interact with a state-of-the-art NLP model and witness the remarkable capabilities of AI-generated text.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
